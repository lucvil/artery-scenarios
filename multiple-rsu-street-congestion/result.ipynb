{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ログからデータを抽出しファイルに格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_log_file(file_path):\n",
    "    events = []\n",
    "    current_event = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # 行が空でない場合\n",
    "                current_event.append(line)\n",
    "            else:\n",
    "                if current_event:  # 現在のセクションが空でない場合\n",
    "                    events.append(''.join(current_event))  # 行を結合してセクションを追加\n",
    "                    current_event = []\n",
    "\n",
    "        # 最後のセクションを追加（空行で区切られない場合も考慮）\n",
    "        if current_event:\n",
    "            events.append(''.join(current_event))\n",
    "\n",
    "    return events\n",
    "\n",
    "def filter_events_by_search_str(strings, search_strings):\n",
    "    \"\"\"\n",
    "    指定された検索文字列のいずれかを含む文字列だけを抽出する関数。\n",
    "\n",
    "    :param strings: 検索対象の文字列のリスト\n",
    "    :param search_strings: 検索する文字列のリスト\n",
    "    :return: 検索文字列のいずれかを含む文字列のリスト\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    for s in strings:\n",
    "        if any(search in s for search in search_strings):\n",
    "            filtered.append(s)\n",
    "    return filtered\n",
    "\n",
    "def write_events_to_file(sections, output_path):\n",
    "    with open(output_path, 'w') as file:\n",
    "        for section in sections:\n",
    "            file.write(section + '\\n')  # セクションの後に空白行を追加\n",
    "\n",
    "\n",
    "events = process_log_file(\"./results/General-#0.elog\")\n",
    "search_strings = [\n",
    "    \"DEBUG:Sending (inet::physicallayer::RadioFrame)GeoNet packet from (artery::VanetRadio)radio\",\n",
    "    \"DEBUG:Computing whether reception is possible\"\n",
    "]\n",
    "filtered_events = filter_events_by_search_str(events, search_strings)\n",
    "write_events_to_file(filtered_events, \"./results/filtered-General-#0.elog\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtered-General-#0.elogからextracted_events.jsonを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_log_file(file_path):\n",
    "    events = []\n",
    "    current_event = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if line.strip():  # Check if the line is not empty\n",
    "                current_event.append(line.strip())\n",
    "            else:\n",
    "                if current_event:\n",
    "                    events.append(current_event)\n",
    "                    current_event = []\n",
    "\n",
    "        # Append the last event if it doesn't end with an empty line\n",
    "        if current_event:\n",
    "            events.append(current_event)\n",
    "\n",
    "    return events\n",
    "\n",
    "\n",
    "def extract_event_id(event):\n",
    "    first_item = event[0]  # Get the first item from the list\n",
    "    if \"E #\" in first_item:\n",
    "        parts = first_item.split()\n",
    "        for i, part in enumerate(parts):\n",
    "            if part == \"E\":  # Check for the \"E\" token\n",
    "                if i + 2 < len(parts):  # Ensure there's a number after \"E #\"\n",
    "                    return parts[i + 2]  # Return the number following \"E #\"\n",
    "    return None  # Return None if \"E #\" pattern is not found\n",
    "\n",
    "\n",
    "def filter_events_by_strings(events):\n",
    "\n",
    "    search_strings = [\"KF\"]\n",
    "\n",
    "    filtered_events = [event for event in events if any(any(substring in line for substring in search_strings) for line in event)]\n",
    "    return filtered_events\n",
    "\n",
    "def extract_following_events_based_on_occurrences(events, search_string):\n",
    "    results = {}\n",
    "    for i, event in enumerate(events[:]):\n",
    "        # Count occurrences of the search string in the current event\n",
    "        count = sum(search_string in line for line in event)\n",
    "        \n",
    "        if count > 0:\n",
    "            # Extract (count - 1) subsequent events\n",
    "            count -= 1  # Adjust to extract one fewer than the count of occurrences\n",
    "            buffer_log_num = 1 # tranmitterのログからすぐにreceiverのログに行かない可能性があるためバッファを設ける\n",
    "            subsequent_events = events[i+1:i+1+count+buffer_log_num] if i+1+count <= len(events) else events[i+1:]\n",
    "\n",
    "            results[extract_event_id(event)] = {}\n",
    "            results[extract_event_id(event)][\"transmitter_event\"] = event\n",
    "            results[extract_event_id(event)][\"receiver_events\"] = subsequent_events\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_transmitter_id_and_timestamp(log_entries):\n",
    "    transmitter_id = None\n",
    "    startTime = None\n",
    "    endTime = None\n",
    "    transmitter_position = None\n",
    "    for entry in log_entries:\n",
    "        if \"transmitterId =\" in entry:\n",
    "            parts = entry.split(',')\n",
    "            # Extract transmitterId and startTime\n",
    "            for part in parts:\n",
    "                if \"transmitterId\" in part:\n",
    "                    transmitter_id = part.split('=')[-1].strip()\n",
    "                if \"startTime\" in part:\n",
    "                    startTime = part.split('=')[-1].strip()\n",
    "                    startTime = float(startTime)\n",
    "                if \"endTime\" in part:\n",
    "                    endTime = part.split('=')[-1].strip()\n",
    "                    endTime = float(endTime)\n",
    "                \n",
    "                \n",
    "                # Attempt to extract position if available\n",
    "                if \"startPosition =\" in entry:\n",
    "                    # Correctly capture the full coordinates in the tuple format\n",
    "                    transmitter_position = entry.split(\"startPosition =\")[1].split('),')[0].strip() + ')'\n",
    "    return transmitter_id, startTime, endTime, transmitter_position\n",
    "\n",
    "\n",
    "# from multiple receiver logs, extract receiver_id and reception possibility\n",
    "def extract_receiver_id_and_reception_possibility(log_entries_2list):\n",
    "    results = {}\n",
    "    receivable_id_list = []\n",
    "\n",
    "    for log_entries in log_entries_2list:\n",
    "        receiver_id = None\n",
    "        position = None\n",
    "        reception_status = \"possible\"\n",
    "        startTime = None\n",
    "        endTime = None\n",
    "\n",
    "        for entry in log_entries:\n",
    "\n",
    "\n",
    "            if \"receiverId =\" in entry:\n",
    "                # Extract the receiver ID from the entry\n",
    "                receiver_id = entry.split('receiverId =')[-1].split(',')[0].strip()\n",
    "            # Attempt to extract position if available\n",
    "            if \"startPosition =\" in entry:\n",
    "                # Correctly capture the full coordinates in the tuple format\n",
    "                position = entry.split(\"startPosition =\")[1].split('),')[0].strip() + ')'\n",
    "            # Update the reception status based on current entry details\n",
    "            if \"reception is impossible\" in entry:\n",
    "                reception_status = \"impossible\"\n",
    "\n",
    "\n",
    "            # if \"startTime\" in entry:\n",
    "            #     startTime = entry.split('=')[-1].strip()\n",
    "            # if \"endTime\" in entry:\n",
    "            #     endTime = entry.split('=')[-1].strip()\n",
    "\n",
    "        if receiver_id is not None:\n",
    "            results[receiver_id] = {\n",
    "                \"position\": position,\n",
    "                \"reception_status\": reception_status\n",
    "            }\n",
    "\n",
    "            if reception_status == \"possible\":\n",
    "                receivable_id_list.append(int(receiver_id))\n",
    "            \n",
    "    # order by receiver_id\n",
    "    results = dict(sorted(results.items(), key=lambda x: x[0]))\n",
    "    receivable_id_list = sorted(set(receivable_id_list))\n",
    "        \n",
    "    return results, receivable_id_list\n",
    "\n",
    "\n",
    "def extract_paramater_values(events_dict):\n",
    "    for key, value in events_dict.items():\n",
    "        transmitter_id, transmitter_startTime, transmitter_endTime,transmitter_position = extract_transmitter_id_and_timestamp(value[\"transmitter_event\"])\n",
    "        receive_results, receivable_id_list = extract_receiver_id_and_reception_possibility(value[\"receiver_events\"])\n",
    "        events_dict[key][\"transmitter_id\"] = transmitter_id\n",
    "        events_dict[key][\"startTime\"] = transmitter_startTime\n",
    "        events_dict[key][\"endTime\"] = transmitter_endTime\n",
    "        events_dict[key][\"transmitter_position\"] = transmitter_position\n",
    "        events_dict[key][\"receiver_results\"] = receive_results\n",
    "        events_dict[key][\"receivable_id_list\"] = receivable_id_list\n",
    "        events_dict[key][\"receivable_id_count\"] = len(receivable_id_list)\n",
    "\n",
    "        # Remove the transmitter_event and receiver_events keys\n",
    "        events_dict[key].pop(\"transmitter_event\")\n",
    "        events_dict[key].pop(\"receiver_events\")\n",
    "    \n",
    "    return events_dict\n",
    "\n",
    "def ensure_directory_exists(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def split_json_by_transmitter_id(data):\n",
    "    # transmitter_idでデータを分割するための辞書\n",
    "    split_data = defaultdict(dict)\n",
    "\n",
    "    # 各エントリをtransmitter_idごとに分割\n",
    "    for key, entry in data.items():\n",
    "        transmitter_id = entry.get('transmitter_id')\n",
    "        if transmitter_id is not None:\n",
    "            if transmitter_id not in split_data:\n",
    "                split_data[transmitter_id] = {}\n",
    "            split_data[transmitter_id][key] = entry\n",
    "\n",
    "    # transmitter_idごとのデータを辞書に格納\n",
    "    result = {id: entries for id, entries in split_data.items()}\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "def main():\n",
    "\n",
    "    speed = 80 # km/hour\n",
    "    events = process_log_file(\"./results/filtered-General-#0.elog\")\n",
    "\n",
    "    # # write event.json file\n",
    "    # with open(\"./results/events.json\", 'w') as f:\n",
    "    #     json.dump(events[:1000], f, indent=4)\n",
    "\n",
    "    log_folder_path = \"./results/speed\" + str(speed) + \"/250vehicle/\"\n",
    "    ensure_directory_exists(log_folder_path)\n",
    "    filtered_events_file_path = log_folder_path + \"filtered_events.json\"\n",
    "    # extracted_events_file_path = log_folder_path + \"/extracted_events.json\"\n",
    "    # print(events)\n",
    "\n",
    "    filtered_string = \"DEBUG:Sending (inet::physicallayer::RadioFrame)GeoNet packet\"\n",
    "    filtered_events = extract_following_events_based_on_occurrences(events, filtered_string)\n",
    "  \n",
    "    # # write to the json file\n",
    "    # with open(filtered_events_file_path, 'w') as f:\n",
    "    #     json.dump(filtered_events, f, indent=4)\n",
    "\n",
    "    # Extract transmitterId, startTime, receiverId, and reception possibility\n",
    "    extracted_events = extract_paramater_values(filtered_events)\n",
    "\n",
    "    # if transmitter_id is not 0 , then the event is deleted form the dictionary\n",
    "    split_events = split_json_by_transmitter_id(extracted_events)\n",
    "\n",
    "    for transmitter_id, split_events_item in split_events.items():        \n",
    "        # write to the json file even if the file is empty\n",
    "        ensure_directory_exists(log_folder_path + str(transmitter_id) + \"/\")\n",
    "        with open(log_folder_path + str(transmitter_id) + \"/extracted_events.json\", 'w') as f:\n",
    "            json.dump(split_events_item, f, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extracted_eventsデータから時刻と通信可能ノードのリスト(receivable_time_id.json)を出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def make_receivable_time_id_file(log_folder_path):\n",
    "\textracted_events_file_path = log_folder_path + \"extracted_events.json\"\n",
    "\n",
    "\twith open(extracted_events_file_path, 'r') as f:\n",
    "\t\textracted_data = json.load(f)\n",
    "\n",
    "\t# 必要な情報を抽出して辞書にまとめる\n",
    "\treceivable_time_id_data = {}\n",
    "\tfor key, value in extracted_data.items():\n",
    "\t\treceivable_time_id_data [value[\"startTime\"]] = value[\"receivable_id_list\"]\n",
    "\t\t\n",
    "\t# write to the json file\n",
    "\twith open( log_folder_path + 'receivable_time_id.json', 'w') as f:\n",
    "\t\tjson.dump(receivable_time_id_data, f, indent=4)\n",
    "\n",
    "\n",
    "# read the json file\n",
    "speed = 80 # km/hour\n",
    "booth_change = True\n",
    "has_multi_proposer = True\n",
    "proposer_list = [0,1,2]\n",
    "validator_num = 250\n",
    "validator_list = [i for i in range(validator_num  + len(proposer_list)) if i not in proposer_list]\n",
    "\n",
    "if booth_change:\n",
    "\tif has_multi_proposer:\n",
    "\t\tfor proposer_id in proposer_list:\n",
    "\t\t\tlog_folder_path = \"./results/speed\" + str(speed) + \"/250vehicle/\" + str(proposer_id) + \"/\"\n",
    "\t\t\tmake_receivable_time_id_file(log_folder_path)\n",
    "\telse:\n",
    "\t\tlog_folder_path = \"./results/speed\" + str(speed) + \"/250vehicle/\"\n",
    "\t\tmake_receivable_time_id_file(log_folder_path)\n",
    "else:\n",
    "\tlog_folder_path = \"./results/speed\" + str(speed) + \"/\"\n",
    "\tmake_receivable_time_id_file(log_folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "receivable_time_idデータから時刻の合間を補完したextended_time_id.jsonを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def make_extended_time_id_file(log_folder_path):\n",
    "\n",
    "\treceivable_time_id_data_path = log_folder_path + 'receivable_time_id.json'\n",
    "\twith open(receivable_time_id_data_path, 'r') as f:\n",
    "\t\treceivable_time_id_data  = json.load(f)\n",
    "\n",
    "\n",
    "\tround_time_id_data = {}\n",
    "\tfor key, value in receivable_time_id_data.items():\n",
    "\t\tround_time_id_data[str(f\"{round(float(key), 2):.2f}\")] = value\n",
    "\n",
    "\n",
    "\t# order the keys\n",
    "\tround_time_id_data = dict(sorted(round_time_id_data.items(), key=lambda x: float(x[0])))\n",
    "\n",
    "\n",
    "\textended_time_id_data = {}\n",
    "\t# 元の辞書のキーと値をループして処理\n",
    "\tfor key, value in round_time_id_data.items():\n",
    "\t\t# 新しいキーを追加\n",
    "\t\textended_time_id_data[key] = value\n",
    "\n",
    "\t\t# 小数部を1ずつ増やしながら、指定された範囲のキーが欠けている場合に追加する\n",
    "\t\tfor decimal in range(1,10):\n",
    "\t\t\tnew_key = f\"{round(float(key) + decimal * 0.01, 2):.2f}\"\n",
    "\t\t\t# 小数点第三位で四捨五入\n",
    "\t\t\tif new_key not in round_time_id_data:  # 新しいキーが元の辞書に存在しない場合\n",
    "\t\t\t\textended_time_id_data[new_key] = value  # 新しいキーを追加し、空のリストを値として設定\n",
    "\t\t\telse:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t# write to the json file\n",
    "\twith open( log_folder_path + 'extended_time_id.json', 'w') as f:\n",
    "\t\tjson.dump(extended_time_id_data, f, indent=4)\n",
    "\n",
    "\n",
    "# read the json file\n",
    "speed = 80 # km/hour\n",
    "booth_change = True\n",
    "has_multi_proposer = True\n",
    "proposer_list = [0,1,2]\n",
    "validator_num = 250\n",
    "validator_list = [i for i in range(validator_num  + len(proposer_list)) if i not in proposer_list]\n",
    "\n",
    "if booth_change:\n",
    "\tif has_multi_proposer:\n",
    "\t\tfor proposer_id in proposer_list:\n",
    "\t\t\tlog_folder_path = \"./results/speed\" + str(speed) + \"/250vehicle/\" + str(proposer_id) + \"/\"\n",
    "\t\t\tmake_extended_time_id_file(log_folder_path)\n",
    "\telse:\n",
    "\t\tlog_folder_path = \"./results/speed\" + str(speed) + \"/250vehicle/\"\n",
    "\t\tmake_extended_time_id_file(log_folder_path)\n",
    "else:\n",
    "\tlog_folder_path = \"./results/speed\" + str(speed) + \"/\"\n",
    "\tmake_extended_time_id_file(log_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vguardで用いるデータに整形(communication_data_for_vguard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = 80 # km/hour\n",
    "booth_change = True\n",
    "has_multi_proposer = True\n",
    "proposer_list = [0,1,2]\n",
    "validator_num = 250\n",
    "validator_list = [i for i in range(validator_num  + len(proposer_list)) if i not in proposer_list]\n",
    "log_folder_path = \"./results/speed\" + str(speed) + \"/250vehicle/\"\n",
    "\n",
    "\n",
    "all_communication_data = {}\n",
    "for proposer_id in proposer_list:\n",
    "    # communication-data.jsonを読み込む\n",
    "    with open(log_folder_path + str(proposer_id) + '/extended_time_id.json', 'r') as file:\n",
    "        proposer_communication_data = json.load(file)\n",
    "    all_communication_data[proposer_id] = proposer_communication_data\n",
    "    with open(log_folder_path + str(proposer_id) + '/communication_node_for_vguard_' + str(proposer_id) + '.json', 'w') as file:\n",
    "        json.dump(all_communication_data[proposer_id], file, indent=4)\n",
    "\n",
    "\n",
    "# validator_listに基づいてバリデータの通信可能なproposerリストを作成\n",
    "validator_output_data = {validator: {} for validator in validator_list}\n",
    "\n",
    "# まず最初に、最初のプロポーザのキーの集合を基準にする\n",
    "common_keys = set(all_communication_data[proposer_list[0]].keys())\n",
    "\n",
    "# proposer_listのすべてのiに対して共通するkeyを取得\n",
    "for i in range(1, len(proposer_list)):\n",
    "    # 各プロポーザのキーとの共通集合を更新\n",
    "    common_keys &= set(all_communication_data[proposer_list[i]].keys())   \n",
    "\n",
    "# common_keysの中身を数値に変換してソート\n",
    "common_keys_as_floats = sorted([float(key) for key in common_keys])\n",
    "common_keys_as_strings = [f\"{key:.2f}\" for key in common_keys_as_floats]\n",
    "\n",
    "\n",
    "for key in common_keys_as_strings:\n",
    "    for validator in validator_list:\n",
    "        # 各バリデータに対して通信可能なプロポーザーリストを収集\n",
    "        validator_communication_list = []\n",
    "        for proposer_id in proposer_list:\n",
    "            # プロポーザーのリストにバリデータが含まれているかをチェック\n",
    "            if validator in all_communication_data[proposer_id][key]:\n",
    "                validator_communication_list.append(proposer_id)\n",
    "        \n",
    "        # 通信可能なプロポーザーリストを順番通りに保存\n",
    "        validator_output_data[validator][key] = validator_communication_list\n",
    "\n",
    "# communication-data-2.json から communication-data-12.json にデータを保存\n",
    "for validator_id in validator_list:\n",
    "    filename = log_folder_path + str(validator_id) + '/communication_node_for_vguard_' + str(validator_id) + '.json'\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(validator_output_data[validator_id], file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOMな自動車でブロックチェーンを作る場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "speed = 80 # km/hour\n",
    "booth_change = True\n",
    "has_multi_proposer = True\n",
    "proposer_list = [0,1,2]\n",
    "validator_num = 250\n",
    "validator_list = [i for i in range(validator_num  + len(proposer_list)) if i not in proposer_list]\n",
    "log_folder_path = \"./results/speed\" + str(speed) + \"/250vehicle/\"\n",
    "\n",
    "\n",
    "all_communication_data = {}\n",
    "for proposer_id in proposer_list:\n",
    "    # communication-data.jsonを読み込む\n",
    "    with open(log_folder_path + str(proposer_id) + '/extended_time_id.json', 'r') as file:\n",
    "        proposer_communication_data = json.load(file)\n",
    "    all_communication_data[proposer_id] = proposer_communication_data\n",
    "\n",
    "# validator_listに基づいてバリデータの通信可能なproposerリストを作成\n",
    "validator_output_data = {validator: {} for validator in validator_list}\n",
    "proposer_output_data = {proposer : {} for proposer in proposer_list }\n",
    "\n",
    "# まず最初に、最初のプロポーザのキーの集合を基準にする\n",
    "common_keys = set(all_communication_data[proposer_list[0]].keys())\n",
    "\n",
    "# proposer_listのすべてのiに対して共通するkeyを取得\n",
    "for i in range(1, len(proposer_list)):\n",
    "    # 各プロポーザのキーとの共通集合を更新\n",
    "    common_keys &= set(all_communication_data[proposer_list[i]].keys())   \n",
    "\n",
    "# common_keysの中身を数値に変換してソート\n",
    "common_keys_as_floats = sorted([float(key) for key in common_keys])\n",
    "common_keys_as_strings = [f\"{key:.2f}\" for key in common_keys_as_floats]\n",
    "\n",
    "old_communication_2list = []\n",
    "now_communication_2list = []\n",
    "random_assigned_list = [[],[],[]]\n",
    "\n",
    "\n",
    "for key in common_keys_as_strings:\n",
    "\n",
    "    # ← 修正：初期化をループ外ではなくここで一度だけ\n",
    "    now_communication_2list = []\n",
    "    for proposer_id in proposer_list:\n",
    "        now_communication_2list.append(all_communication_data[proposer_id][key])\n",
    "\n",
    "    # random_assigned更新\n",
    "    if old_communication_2list != now_communication_2list:\n",
    "        # ← 修正：三重リストをやめ、3分割の一次元リストに\n",
    "        u = list({x for s in now_communication_2list for x in s})\n",
    "        random.shuffle(u)\n",
    "        random_assigned_list = [sorted(u[i::3]) for i in range(3)]\n",
    "        old_communication_2list = now_communication_2list\n",
    "\n",
    "    for proposer_id in proposer_list:\n",
    "        proposer_output_data[proposer_id][key] = random_assigned_list[proposer_id]\n",
    "\n",
    "    # ← 修正：フラットにして整数をキーにする\n",
    "    pos = {v: i+1 for i, sub in enumerate(random_assigned_list) for v in sub}  # 要素→「何個目のリスト」(1始まり)\n",
    "    for validator_id in validator_list:\n",
    "        if pos.get(validator_id) != None:\n",
    "            validator_output_data[validator_id][key] = [pos.get(validator_id) -1]\n",
    "        else:\n",
    "            validator_output_data[validator_id][key] = []\n",
    "    \n",
    "\n",
    "# communication-data-2.json から communication-data-12.json にデータを保存\n",
    "for proposer_id in proposer_list:\n",
    "    filename = log_folder_path + str(proposer_id) + '/random_decided_participant_node_' + str(proposer_id) + '.json'\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(proposer_output_data[proposer_id], file, indent=4)   \n",
    "\n",
    "\n",
    "for validator_id in validator_list:\n",
    "    filename = log_folder_path + str(validator_id) + '/random_decided_participant_node_' + str(validator_id) + '.json'\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(validator_output_data[validator_id], file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10秒ごとに再抽選"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import math\n",
    "\n",
    "speed = \"70_30\"\n",
    "booth_change = True\n",
    "has_multi_proposer = True\n",
    "proposer_list = [0,1,2]\n",
    "validator_num = 250\n",
    "validator_list = [i for i in range(validator_num  + len(proposer_list)) if i not in proposer_list]\n",
    "log_folder_path = \"./results/speed\"+speed+\"/250vehicle/\"\n",
    "\n",
    "all_communication_data = {}\n",
    "for proposer_id in proposer_list:\n",
    "    # communication-data.jsonを読み込む\n",
    "    with open(log_folder_path + str(proposer_id) + '/extended_time_id.json', 'r') as file:\n",
    "        proposer_communication_data = json.load(file)\n",
    "    all_communication_data[proposer_id] = proposer_communication_data\n",
    "\n",
    "# validator_listに基づいてバリデータの通信可能なproposerリストを作成\n",
    "validator_output_data = {validator: {} for validator in validator_list}\n",
    "proposer_output_data = {proposer : {} for proposer in proposer_list }\n",
    "\n",
    "# まず最初に、最初のプロポーザのキーの集合を基準にする\n",
    "common_keys = set(all_communication_data[proposer_list[0]].keys())\n",
    "\n",
    "# proposer_listのすべてのiに対して共通するkeyを取得\n",
    "for i in range(1, len(proposer_list)):\n",
    "    # 各プロポーザのキーとの共通集合を更新\n",
    "    common_keys &= set(all_communication_data[proposer_list[i]].keys())\n",
    "\n",
    "# common_keysの中身を数値に変換してソート\n",
    "common_keys_as_floats = sorted([float(key) for key in common_keys])\n",
    "common_keys_as_strings = [f\"{key:.2f}\" for key in common_keys_as_floats]\n",
    "\n",
    "# 3秒刻み再抽選用の状態\n",
    "last_bucket = None                 # 直前の 3 秒バケット（整数）\n",
    "random_assigned_base = [[], [], []]  # 直近の再抽選で決めた基準割当（可用集合に対して）\n",
    "\n",
    "for key in common_keys_as_strings:\n",
    "    # 現在時刻と3秒バケットを計算\n",
    "    t = float(key)\n",
    "    bucket = int(t // 10)\n",
    "\n",
    "    # この時刻に通信可のノード集合（各プロポーザの集合の和）\n",
    "    now_communication_2list = []\n",
    "    for proposer_id in proposer_list:\n",
    "        now_communication_2list.append(all_communication_data[proposer_id][key])\n",
    "    u_current = sorted({x for s in now_communication_2list for x in s})\n",
    "\n",
    "    # 3秒バケットが変わったら再抽選\n",
    "    if bucket != last_bucket:\n",
    "        u = list(u_current)  # バケット開始時点の可用集合で抽選\n",
    "        random.shuffle(u)\n",
    "        random_assigned_base = [sorted(u[i::3]) for i in range(3)]\n",
    "        last_bucket = bucket\n",
    "\n",
    "    # 出力は常に「現在の可用集合」に限定\n",
    "    # → 同一バケット内でも可用集合が時刻で増減しても、現在いるノードだけを割り当てる\n",
    "    assigned_now = [\n",
    "        sorted([v for v in random_assigned_base[i] if v in u_current])\n",
    "        for i in range(3)\n",
    "    ]\n",
    "\n",
    "    # proposer側の出力（この時刻の可用ノードのみ）\n",
    "    for proposer_id in proposer_list:\n",
    "        proposer_output_data[proposer_id][key] = assigned_now[proposer_id]\n",
    "\n",
    "    # validator側の出力（どの proposer に割当か：0/1/2、未割当なら []）\n",
    "    pos = {v: i for i, sub in enumerate(assigned_now) for v in sub}  # 要素→どのリストか(0始まり)\n",
    "    for validator_id in validator_list:\n",
    "        if validator_id in pos:\n",
    "            validator_output_data[validator_id][key] = [pos[validator_id]]\n",
    "        else:\n",
    "            validator_output_data[validator_id][key] = []\n",
    "\n",
    "# 保存\n",
    "for proposer_id in proposer_list:\n",
    "    filename = log_folder_path + str(proposer_id) + '/10s_interval_random_decided_participant_node_' + str(proposer_id) + '.json'\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(proposer_output_data[proposer_id], file, indent=4)\n",
    "\n",
    "for validator_id in validator_list:\n",
    "    filename = log_folder_path + str(validator_id) + '/10s_interval_random_decided_participant_node_' + str(validator_id) + '.json'\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(validator_output_data[validator_id], file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
